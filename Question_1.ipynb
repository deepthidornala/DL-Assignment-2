{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOknyMsKGksGuPqthqDuYVd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deepthidornala/DL-Assignment-2/blob/main/Question_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOcbDt3C0Twh",
        "outputId": "4a6358ff-0e1f-493a-a273-eb59349ec1f4",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │      \u001b[38;5;34m3,456\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │      \u001b[38;5;34m8,448\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)         │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),     │    \u001b[38;5;34m394,240\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),      │            │                   │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]      │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     │    \u001b[38;5;34m394,240\u001b[0m │ embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
              "│                     │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m],       │\n",
              "│                     │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m]        │\n",
              "│                     │ \u001b[38;5;34m256\u001b[0m)]             │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m66\u001b[0m)  │     \u001b[38;5;34m16,962\u001b[0m │ lstm_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,456</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,448</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)         │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">394,240</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),      │            │                   │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]      │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">394,240</span> │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>],       │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>]        │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]             │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,962</span> │ lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m817,346\u001b[0m (3.12 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">817,346</span> (3.12 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m817,346\u001b[0m (3.12 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">817,346</span> (3.12 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m130/553\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:37\u001b[0m 231ms/step - accuracy: 0.6584 - loss: 1.6900"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, GRU, SimpleRNN, Dense, Embedding\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Configuration class for flexible model architecture\n",
        "class ModelConfig:\n",
        "    def __init__(self):\n",
        "        # Embedding dimensions\n",
        "        self.embedding_dim = 128\n",
        "\n",
        "        # RNN layer configuration\n",
        "        self.rnn_type = 'lstm'  # 'lstm', 'gru', or 'rnn'\n",
        "        self.hidden_size = 256\n",
        "        self.num_layers = 1\n",
        "\n",
        "        # Training parameters\n",
        "        self.batch_size = 64\n",
        "        self.epochs = 30\n",
        "        self.validation_split = 0.2\n",
        "\n",
        "# Load and preprocess data\n",
        "def load_data(path):\n",
        "    df = pd.read_csv(path, sep='\\t', header=None)\n",
        "    df.dropna(inplace=True)\n",
        "    return list(zip(df[1], df[0]))  # Latin, Devanagari\n",
        "\n",
        "# Load train and test data\n",
        "train_pairs = load_data('/content/hi.translit.sampled.train.tsv')\n",
        "test_pairs = load_data('/content/hi.translit.sampled.test.tsv')\n",
        "\n",
        "# Prepare text data\n",
        "input_texts = [inp.lower() for inp, _ in train_pairs]\n",
        "target_texts = ['\\t' + tgt + '\\n' for _, tgt in train_pairs]\n",
        "\n",
        "# Build vocabulary\n",
        "input_chars = sorted(set(''.join(input_texts)))\n",
        "target_chars = sorted(set(''.join(target_texts)))\n",
        "input_token_index = {char: i+1 for i, char in enumerate(input_chars)}  # 0 reserved for padding\n",
        "target_token_index = {char: i+1 for i, char in enumerate(target_chars)}\n",
        "\n",
        "reverse_input_char_index = {i: char for char, i in input_token_index.items()}\n",
        "reverse_target_char_index = {i: char for char, i in target_token_index.items()}\n",
        "\n",
        "# Calculate sequence lengths\n",
        "max_encoder_seq_length = max(len(txt) for txt in input_texts)\n",
        "max_decoder_seq_length = max(len(txt) for txt in target_texts)\n",
        "\n",
        "# Tokenize and pad sequences\n",
        "def tokenize(texts, token_index, max_len):\n",
        "    sequences = [[token_index.get(char, 0) for char in text] for text in texts]\n",
        "    return pad_sequences(sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "encoder_input_data = tokenize(input_texts, input_token_index, max_encoder_seq_length)\n",
        "decoder_input_data = tokenize(target_texts, target_token_index, max_decoder_seq_length)\n",
        "\n",
        "# Prepare decoder target data (shifted by one)\n",
        "decoder_target_data = np.zeros_like(decoder_input_data)\n",
        "decoder_target_data[:, :-1] = decoder_input_data[:, 1:]\n",
        "\n",
        "# Split into train and validation\n",
        "(enc_train, enc_val,\n",
        " dec_in_train, dec_in_val,\n",
        " dec_tgt_train, dec_tgt_val) = train_test_split(\n",
        "    encoder_input_data, decoder_input_data, decoder_target_data,\n",
        "    test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Build the model\n",
        "def build_model(config, num_encoder_tokens, num_decoder_tokens):\n",
        "    # Encoder\n",
        "    encoder_inputs = Input(shape=(None,))\n",
        "    enc_emb = Embedding(num_encoder_tokens, config.embedding_dim)(encoder_inputs)\n",
        "\n",
        "    # Choose RNN type\n",
        "    if config.rnn_type.lower() == 'lstm':\n",
        "        RNN = LSTM\n",
        "    elif config.rnn_type.lower() == 'gru':\n",
        "        RNN = GRU\n",
        "    else:\n",
        "        RNN = SimpleRNN\n",
        "\n",
        "    # Encoder RNN\n",
        "    encoder_rnn = RNN(config.hidden_size, return_state=True)\n",
        "    encoder_outputs, *encoder_states = encoder_rnn(enc_emb)\n",
        "\n",
        "    # Decoder\n",
        "    decoder_inputs = Input(shape=(None,))\n",
        "    dec_emb = Embedding(num_decoder_tokens, config.embedding_dim)(decoder_inputs)\n",
        "\n",
        "    # Decoder RNN\n",
        "    decoder_rnn = RNN(config.hidden_size, return_sequences=True, return_state=True)\n",
        "    decoder_outputs, *decoder_states = decoder_rnn(\n",
        "        dec_emb, initial_state=encoder_states\n",
        "    )\n",
        "\n",
        "    # Dense layer\n",
        "    decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "    # Full model\n",
        "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# Initialize configuration\n",
        "config = ModelConfig()\n",
        "num_encoder_tokens = len(input_token_index) + 1  # +1 for padding\n",
        "num_decoder_tokens = len(target_token_index) + 1\n",
        "\n",
        "# Build and train model\n",
        "model = build_model(config, num_encoder_tokens, num_decoder_tokens)\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(\n",
        "    [enc_train, dec_in_train],\n",
        "    np.expand_dims(dec_tgt_train, -1),\n",
        "    batch_size=config.batch_size,\n",
        "    epochs=config.epochs,\n",
        "    validation_data=(\n",
        "        [enc_val, dec_in_val],\n",
        "        np.expand_dims(dec_tgt_val, -1)\n",
        "    )\n",
        ")\n",
        "\n",
        "# Build inference models\n",
        "def build_inference_models(model, config):\n",
        "    # Encoder inference model\n",
        "    encoder_inputs = model.input[0]\n",
        "    encoder_outputs, *encoder_states = model.layers[4].output  # RNN layer\n",
        "    encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "    # Decoder inference model\n",
        "    decoder_inputs = model.input[1]\n",
        "    decoder_embedding = model.layers[3]  # Decoder embedding\n",
        "\n",
        "    # State inputs\n",
        "    decoder_state_inputs = [\n",
        "        Input(shape=(config.hidden_size,))\n",
        "        for _ in range(len(encoder_states))\n",
        "    ]\n",
        "\n",
        "    # Decoder RNN\n",
        "    decoder_rnn = model.layers[5]\n",
        "    dec_emb = decoder_embedding(decoder_inputs)\n",
        "    decoder_outputs, *decoder_states = decoder_rnn(\n",
        "        dec_emb, initial_state=decoder_state_inputs\n",
        "    )\n",
        "\n",
        "    # Dense layer\n",
        "    decoder_dense = model.layers[6]\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "    decoder_model = Model(\n",
        "        [decoder_inputs] + decoder_state_inputs,\n",
        "        [decoder_outputs] + decoder_states\n",
        "    )\n",
        "\n",
        "    return encoder_model, decoder_model\n",
        "\n",
        "encoder_model, decoder_model = build_inference_models(model, config)\n",
        "\n",
        "# Decode sequence function\n",
        "def decode_sequence(input_seq, encoder_model, decoder_model,\n",
        "                   target_token_index, reverse_target_char_index,\n",
        "                   max_decoder_seq_length):\n",
        "    # Encode the input sequence\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = target_token_index['\\t']\n",
        "\n",
        "    # Sampling loop\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, *states_value = decoder_model.predict(\n",
        "            [target_seq] + states_value\n",
        "        )\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index.get(sampled_token_index, '')\n",
        "\n",
        "        # Exit condition\n",
        "        if sampled_char == '\\n' or len(decoded_sentence) > max_decoder_seq_length:\n",
        "            stop_condition = True\n",
        "        else:\n",
        "            decoded_sentence += sampled_char\n",
        "\n",
        "        # Update target sequence\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "    return decoded_sentence\n",
        "\n",
        "# Prepare test data\n",
        "test_input_texts = [inp.lower() for inp, _ in test_pairs]\n",
        "test_target_texts = [tgt for _, tgt in test_pairs]\n",
        "\n",
        "test_encoder_input = tokenize(test_input_texts, input_token_index, max_encoder_seq_length)\n",
        "\n",
        "# Evaluate on test set\n",
        "correct = 0\n",
        "total = len(test_input_texts)\n",
        "predictions = []\n",
        "\n",
        "for i in range(total):\n",
        "    input_seq = test_encoder_input[i:i+1]\n",
        "    decoded = decode_sequence(\n",
        "        input_seq, encoder_model, decoder_model,\n",
        "        target_token_index, reverse_target_char_index,\n",
        "        max_decoder_seq_length\n",
        "    )\n",
        "    predictions.append((test_input_texts[i], test_target_texts[i], decoded))\n",
        "    if decoded == test_target_texts[i]:\n",
        "        correct += 1\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f\"\\nTest Accuracy: {accuracy:.2%}\")\n",
        "\n",
        "# Show sample predictions\n",
        "print(\"\\nSample Predictions:\")\n",
        "for i in range(min(10, len(predictions))):\n",
        "    print(f\"Input: {predictions[i][0]}\")\n",
        "    print(f\"Target: {predictions[i][1]}\")\n",
        "    print(f\"Predicted: {predictions[i][2]}\")\n",
        "    print()"
      ]
    }
  ]
}