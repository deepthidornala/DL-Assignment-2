# -*- coding: utf-8 -*-
"""Question_2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AxcnvLDdbBhPjIwdmKdT8ZKdl72HHmYH
"""

!pip install transformers datasets torch pandas

import os
os.environ["WANDB_DISABLED"] = "true"

import torch
from transformers import (
    GPT2Tokenizer,
    GPT2LMHeadModel,
    Trainer,
    TrainingArguments,
    DataCollatorForLanguageModeling,
    pipeline
)
from datasets import load_dataset
import pandas as pd

with open("/content/bruno-mars.txt", "r", encoding="utf-8") as f:
    lyrics = f.read()

lyrics = "\n".join([line.strip() for line in lyrics.split("\n") if line.strip()])

with open("processed_lyrics.txt", "w", encoding="utf-8") as f:
    f.write(lyrics)

dataset = load_dataset("text", data_files={"train": "processed_lyrics.txt"})

tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token


def tokenize_function(examples):
    return tokenizer(
        examples["text"],
        padding="max_length",
        truncation=True,
        max_length=128,
        return_tensors="pt"
    )

tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=["text"])


model = GPT2LMHeadModel.from_pretrained("gpt2")
model.resize_token_embeddings(len(tokenizer))

training_args = TrainingArguments(
    output_dir="./gpt2-bruno-mars",
    overwrite_output_dir=True,
    num_train_epochs=5,
    per_device_train_batch_size=4,
    save_steps=10_000,
    save_total_limit=2,
    prediction_loss_only=True,
    learning_rate=5e-5,
    warmup_steps=500,
    logging_dir="./logs",
    report_to="none",
    logging_steps=500
)


data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False
)

trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=tokenized_datasets["train"],
)

print("Starting training...")
trainer.train()
print("Training completed!")

trainer.save_model("./gpt2-bruno-mars")
tokenizer.save_pretrained("./gpt2-bruno-mars")

generator = pipeline(
    "text-generation",
    model="./gpt2-bruno-mars",
    tokenizer=tokenizer,
    device=0 if torch.cuda.is_available() else -1

prompts = [
    "Tonight I'm gonna give you all my love",
    "Girl, you're amazing just the way you are",
    "Don't believe me just watch",
    "I would catch a grenade for you"
]

print("\n Bruno Mars Style Lyrics Generation \n")
for prompt in prompts:
    print(f"Prompt: '{prompt}'")
    outputs = generator(
        prompt,
        max_length=150,
        num_return_sequences=1,
        temperature=0.9,
        top_k=50,
        top_p=0.95,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id
    )
    print("Generated Lyrics:")
    print(outputs[0]["generated_text"])
    print("\n" + "="*80 + "\n")

